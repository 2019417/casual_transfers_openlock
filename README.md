# å››ä¸ªæ ¸æ¡ƒå¤§ä½œä¸š

ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®ğŸ®


## 1. ç¯å¢ƒé…ç½® :wrench:  

å…ˆå®‰è£…åŒ…
```bash
pip install -r requirements.txt
pip install openlockenv-0.0.4-py3-none-any.whl
```

å¸è½½åŒ…
```bash
pip uninstall openlockenv
```

```python
import gymnasium as gym
import openlockenv #ä¸åŠ ä¼šæŠ¥é”™

# ä¸æ”¯æŒrender
# pattern: CC3,CC4,CE3,CE4
# max_steps: default 3, åŒ…æ‹¬å¼€é—¨æœ€å¤šåŠ¨3æ¬¡
# size: default 7 ,7 ä¸ª lever ,ä¸€é—ªé—¨
# seed: default 0
max_step = 5
env = gym.make(id='openlockenv/OpenlockEnv-v0',pattern='CC3',seed=1,max_step=max_step)
#observation, info = env.reset()
#solution = info['solution'] # the solution of the environment
#print(solution)
episode = 10
for e in range(episode):
    print("------" + str(e)+"------")
    observation, info = env.reset()
    solution = info['solution']  # the solution of the environment
    print(solution)
    episode_over = False
    i = 0
    while(not episode_over):
        action = env.action_space.sample() # agent policy that uses the observation and info
        print(f"action_num_{i}:",action)
        i+=1
        observation, reward, terminated, truncated, info = env.step(action) #
        print("obs:",observation,"reward:",reward,"terminated:",terminated, "truncated:",truncated)
        episode_over = terminated or truncated
        print(episode_over)
        if episode_over:
            break

env.close()
```
## 2. OpenLockç¯å¢ƒ
![](https://camo.githubusercontent.com/9e206156f9295b836f2e661d471d76f36828d6d00a111d1d34a7b11c0b0a495b/687474703a2f2f7777772e6d6a65646d6f6e64732e636f6d2f70726f6a656374732f4f70656e4c6f636b2f436f6753636931385f6f70656e6c6f636b5f736f6c7574696f6e732e676966)

a. åŒ…å«7ä¸ªæ†å’Œä¸€ä¸ªé—¨ã€‚æ†çš„é¢œè‰²åˆ†ä¸ºæµ…ç°è‰²å’Œæ·±ç°è‰²ï¼Œæµ…ç°è‰²çš„æ†æ°¸è¿œé”ã€‚éœ€è¦æŒ‰ä¸€å®šé¡ºåºæ¨æ·±ç°è‰²çš„æ†ä»¥è§£é”é—¨ï¼Œç„¶åæ‰èƒ½æ¨å¼€é—¨ã€‚å…¶ä¸­é¢œè‰²æ˜¯èƒ½è¢«è§‚å¯Ÿåˆ°çš„ã€‚æ‰€ä»¥åŸæ–‡æåˆ°çš„abstract-levelæ˜¯è¿™ä¸ªè§£é”é€»è¾‘ï¼Œè€Œinstance-levelæ˜¯é¢œè‰²ä½ç½®å±æ€§çš„å¯¹åº”å…³ç³»ã€‚obs(16ç»´) = {7*(status_lever) + 7*(color_lever)+door_open+door_lock}ã€‚

b. éœ€è¦æ‰¾åˆ°æ‰€æœ‰çš„è§£å†³æ–¹æ³•æ‰èƒ½é€ƒç¦»æˆ¿é—´ã€‚æ–°çš„æˆ¿é—´åªæœ‰æ†çš„ä½ç½®å˜äº†ï¼Œç›¸åº”æ†çš„å±æ€§å’Œé—¨è§£é”çš„é€»è¾‘æ²¡æœ‰æ”¹å˜ã€‚ä¸€æ¬¡éªŒè¯agentçš„causal learningèƒ½åŠ›ã€‚

c. ä¸€æ¬¡attemptåªæœ‰3stepsï¼ŒåŒ…å«å¼€é—¨ã€‚ä»¥ä¸Šå›¾ä¸ºä¾‹ï¼Œ$L_0$åœ¨$L_1$æˆ–$L_2$æ¨å¼€ä¹‹å‰åº”è¯¥æ˜¯è¢«é”ä½ï¼Œæ¨ä¸åŠ¨çš„ï¼ŸåŸæ–‡æ‰€ä¸¾çš„ä¾‹å­æ¨ä¸Šæ†å¯¼è‡´ä¸‹æ†è¢«æ¨å¼€ä¸çŸ¥é“æ˜¯ä¸æ˜¯ç¬”è¯¯ï¼Ÿæ‰€ä»¥ä¸ªæœ€ä½³çš„agentåœ¨ä¸€ä¸ªæˆ¿é—´æœ€å¤šéœ€è¦N+1ä¸ªattemptsï¼ˆNä¸ªè§£é”æ–¹æ³•ï¼‰ã€‚ç¬¬ä¸€æ¬¡æŒ¨ä¸ªæ¨æ·±ç°è‰²çš„æ†ç¡®å®šå…¶åœ¨å›¾é‡Œçš„ç¬¬ä¸€å±‚è¿˜æ˜¯ç¬¬äºŒå±‚ï¼Œç„¶ååé¢Næ¬¡ç›´æ¥æ‰¾åˆ°Nä¸ªè§£é”æ–¹æ³•ã€‚
![b](figs/1.png)

## 3. å®éªŒæ–¹æ³•

### a. Training
è®­ç»ƒé˜¶æ®µåªåŒ…å«3ä¸ªæ·±ç°è‰²æ†çš„åœºæ™¯ã€‚æ¯å®Œæˆä¸€ä¸ªtrialï¼ˆæ‰¾åˆ°æ‰€æœ‰è§£é”æ–¹æ³•æˆ–è€…attemptçš„æ¬¡æ•°ç”¨å®Œï¼‰ï¼Œæ¢åˆ°ä¸‹ä¸€ä¸ªä½ç½®ä¸ä¸€æ ·ä½†è§£é”é€»è¾‘ç›¸åŒçš„æˆ¿é—´è¿›è¡Œä¸‹ä¸€ä¸ªtrialã€‚
### b. Test
#### congruent cases
CE3-CE4 CC3-CC4 (train on 3,test on 4)
#### incongruent cases
CC3-CE4 CE3-CC4 (train on 3,test on 4)
#### baseline
CC4-CC4 CE4-CE4 (train on 4,test on 4)

### c. Experimental Setup
##### transfer-based agentï¼ˆæœ¬æ–‡æå‡ºçš„ï¼‰
(1) a training trial æœ€å¤š30 attemptsã€‚æ¯æ¬¡trialçš„æˆ¿é—´å³æ†çš„ä½ç½®éƒ½ä¸åŒï¼Œç›¸åŒçš„æˆ¿é—´åªçœ‹ä¸€æ¬¡ã€‚

(2) æµ‹è¯•æ—¶å¯¹æ¯ä¸ªæˆ¿é—´ä¹Ÿåªæœ‰1æ¬¡æœºä¼šï¼Œ30ä¸ªattemptã€‚
##### Model-free RL agent
(1) a training trial æœ€å¤š700 attemptsã€‚å¯ä»¥é‡å¤åœ°çœ‹ç›¸åŒçš„æˆ¿é—´ï¼ˆ200æ¬¡ï¼‰ã€‚

(2) æµ‹è¯•æ—¶å¯¹æ¯ä¸ªæˆ¿é—´ä¹Ÿæœ‰200æ¬¡æœºä¼šï¼Œ700ä¸ªattemptã€‚ä»€ä¹ˆæ˜¯all six 3-lever and all five 4-leverï¼Ÿ

(3) reward: å¯¹æ¯ä¸ªè§£å†³æ–¹æ³•åªåœ¨ç¬¬ä¸€æ¬¡æ‰¾åˆ°æ—¶æ‰ç»™ä¸ªå¥–åŠ±ã€‚
![2](figs/2.png)
